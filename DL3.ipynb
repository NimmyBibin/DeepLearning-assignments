{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0c53f9",
   "metadata": {},
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcab706",
   "metadata": {},
   "source": [
    "No, it is not recommended to initialize all the weights to the same value, even if that value is randomly selected using He initialization. Initializing all the weights to the same value can result in the network getting stuck in a symmetrical configuration, where all neurons in a layer have the same output and the gradients become the same during backpropagation. This can slow down the learning process and prevent the network from converging to an optimal solution.\n",
    "\n",
    "To avoid this problem, it is recommended to initialize the weights randomly but with some constraints to ensure that the variance of the outputs of each neuron remains the same across all layers. He initialization is one such method that ensures that the variance of the outputs remains constant across the layers, but it also involves randomly initializing the weights using a Gaussian distribution with a mean of zero and a standard deviation of $\\sqrt{\\frac{2}{n}}$, where n is the number of input connections to each neuron. This ensures that the weights are small enough to prevent saturation of the activation functions while being large enough to enable the gradients to flow during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c883a15",
   "metadata": {},
   "source": [
    "2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec915a",
   "metadata": {},
   "source": [
    "Yes, it is generally considered acceptable to initialize the bias terms to 0. In fact, many commonly used initialization methods, such as Xavier initialization and He initialization, set the bias terms to 0. This is because the role of the bias term is to shift the activation function, and initializing it to 0 ensures that the activation function is centered at the origin.\n",
    "\n",
    "However, in some cases, initializing the bias terms to non-zero values may improve the performance of the network, especially if the data is not centered at the origin. In such cases, it may be beneficial to initialize the bias terms to a small constant value, such as 0.1 or -0.1. This can help the network converge more quickly by providing an initial bias towards the correct output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea9e9c",
   "metadata": {},
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b1260",
   "metadata": {},
   "source": [
    "The SELU (Scaled Exponential Linear Unit) activation function has several advantages over the ReLU (Rectified Linear Unit) activation function:\n",
    "\n",
    "1. SELU is a smooth function that is differentiable everywhere, whereas ReLU has a non-differentiable point at 0. This means that SELU can be used with optimization algorithms that require smooth gradients, such as gradient descent, while ReLU may require special treatment, such as sub-gradient methods.\n",
    "\n",
    "2. SELU has a self-normalizing property that helps to maintain the variance of the activations close to 1, which can help to mitigate the vanishing/exploding gradients problem that can occur in deep neural networks. This means that SELU can be used with deeper architectures without the need for special initialization schemes or other techniques to stabilize training.\n",
    "\n",
    "3. SELU has been shown to outperform ReLU on a number of benchmarks, including image classification, speech recognition, and machine translation. This suggests that SELU may be a more powerful activation function in certain contexts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ad980",
   "metadata": {},
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a5829",
   "metadata": {},
   "source": [
    "The choice of activation function depends on several factors, including the nature of the data, the architecture of the network, and the specific task at hand. Here are some guidelines on when to use different activation functions:\n",
    "\n",
    "1. SELU: This activation function is a good choice for deep neural networks, especially when using the self-normalizing property of the function can help to mitigate the vanishing/exploding gradients problem. However, it is not always the best choice and should be used with caution.\n",
    "\n",
    "2. Leaky ReLU (and its variants): This activation function is a good choice when using ReLU results in a large number of dead neurons (neurons that never activate). The variants, like Parametric ReLU (PReLU), allow the slope of the negative part to be learned and provide a wider range of behavior than the original Leaky ReLU.\n",
    "\n",
    "3. ReLU: This activation function is the most commonly used activation function in deep learning and is a good default choice for many tasks. It is particularly effective for image classification tasks, where it often outperforms other activation functions.\n",
    "\n",
    "4. Tanh: This activation function is a good choice when working with data that has a strong negative/positive bias. Tanh is centered around 0, so it can help to balance the activations in the network and prevent saturation of the weights.\n",
    "\n",
    "5. Logistic (sigmoid): This activation function is often used for binary classification tasks, where the output of the network is a probability value between 0 and 1. It can also be used for multi-class classification problems, although it is not as commonly used as softmax.\n",
    "\n",
    "6. Softmax: This activation function is used in the output layer of the network for multi-class classification problems. It takes a set of inputs and returns a probability distribution over the classes. Softmax is often used in conjunction with cross-entropy loss to train the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0b8c9",
   "metadata": {},
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b028aa9",
   "metadata": {},
   "source": [
    "When the momentum hyperparameter is set too close to 1 (e.g., 0.99999), the optimizer's learning rate for a particular weight will decrease only very gradually when the weight's gradient changes direction, which means the optimizer will continue to move in the previous direction for a long time. This can cause the optimizer to overshoot the minimum of the cost function and bounce back and forth. This results in slow convergence and even divergence, which leads to unstable training. In summary, setting the momentum hyperparameter too close to 1 can slow down the training process, resulting in slow convergence, instability, and sometimes divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978877a",
   "metadata": {},
   "source": [
    "6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eceabd",
   "metadata": {},
   "source": [
    "Here are three ways to produce a sparse model:\n",
    "\n",
    "1. L1 regularization: This regularization technique involves adding the sum of absolute values of weights to the cost function. This encourages the model to learn a sparse set of weights by driving many of them to zero, leaving only the most important ones. \n",
    "\n",
    "2. Dropout: This technique randomly drops out (sets to zero) some fraction of the neurons in each training iteration. This forces the remaining neurons to become more robust and not rely too much on any one particular input. As a result, the model becomes more sparse.\n",
    "\n",
    "3. Data-driven sparsity: This approach involves removing unnecessary or irrelevant features from the input data. By removing unimportant features, the model can become more focused on the relevant features, resulting in a sparser model. Techniques like Principal Component Analysis (PCA) or Independent Component Analysis (ICA) can be used to identify and remove such features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871db6d",
   "metadata": {},
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3c73b",
   "metadata": {},
   "source": [
    "Yes, dropout can slow down training because it involves randomly dropping out neurons during each training iteration, which can increase the number of iterations needed to reach convergence. However, it can also help prevent overfitting, which can ultimately speed up training by improving generalization performance.\n",
    "\n",
    "During inference (i.e., making predictions on new instances), dropout is typically turned off, so it does not slow down inference.\n",
    "\n",
    "MC Dropout (Monte Carlo Dropout) is a variation of dropout that can be used for Bayesian modeling and uncertainty estimation. It involves running dropout during inference and taking multiple predictions with different dropout masks, then averaging them to obtain the final prediction. This can be computationally expensive and may slow down inference, but it can provide valuable information about the model's uncertainty and improve its predictive performance in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec9b63",
   "metadata": {},
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "\n",
    "\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect\n",
    "training speed?\n",
    "\n",
    "\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).\n",
    "\n",
    "\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2636c7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m]))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=5e-5), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=20)])\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=5e-4), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=20)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d3a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
