{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbc5fdf",
   "metadata": {},
   "source": [
    "1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326b50a",
   "metadata": {},
   "source": [
    "An artificial neuron, also known as a perceptron, is a basic building block of artificial neural networks. It is modeled after the structure and function of biological neurons, but simplified for use in computer systems. \n",
    "\n",
    "Like a biological neuron, an artificial neuron receives input signals from other neurons or external sources, processes the information, and generates an output signal. The input signals are multiplied by weights, which represent the strength of the connection between the neurons. The weighted input signals are then summed together and passed through an activation function, which determines the output of the neuron. \n",
    "\n",
    "The structure of an artificial neuron typically consists of three main components: \n",
    "\n",
    "1. Inputs: The input layer receives input signals from external sources or other neurons. Each input signal is multiplied by a weight, which determines the strength of the connection between the input and the neuron.\n",
    "\n",
    "2. Activation function: The activation function determines the output of the neuron based on the weighted sum of the input signals. The output can be binary, continuous, or a combination of both.\n",
    "\n",
    "3. Bias: The bias is a constant value added to the weighted sum of the input signals before passing through the activation function. It allows the neuron to shift the output towards a certain direction, even when the input signals are zero.\n",
    "\n",
    "The artificial neuron is similar to a biological neuron in that it receives input signals, processes information, and generates an output signal. However, the artificial neuron is much simpler in structure and function than a biological neuron, which contains many more components and operates through complex electrochemical processes. The basic principle of information processing, however, is similar in both types of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9165c",
   "metadata": {},
   "source": [
    "2.\tWhat are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42310c54",
   "metadata": {},
   "source": [
    "Activation functions are a key component of artificial neural networks that determine the output of a neuron based on the input signals. There are several types of activation functions that are popularly used in neural networks. Here are some of the most common ones:\n",
    "\n",
    "1. Sigmoid function: The sigmoid function is a non-linear activation function that maps the input to a value between 0 and 1. It is defined as: f(x) = 1 / (1 + e^-x). The sigmoid function is commonly used in binary classification tasks, where the output is binary (0 or 1).\n",
    "\n",
    "2. ReLU function: The rectified linear unit (ReLU) function is a popular activation function used in deep learning. It is defined as: f(x) = max(0, x). The ReLU function is computationally efficient and has been shown to perform well in many deep learning applications. It is especially useful in deep neural networks, where it helps to mitigate the vanishing gradient problem.\n",
    "\n",
    "3. Tanh function: The hyperbolic tangent (tanh) function is similar to the sigmoid function, but maps the input to a value between -1 and 1. It is defined as: f(x) = (e^x - e^-x) / (e^x + e^-x). The tanh function is commonly used in multi-class classification tasks.\n",
    "\n",
    "4. Softmax function: The softmax function is used in multi-class classification tasks, where the output is a probability distribution over multiple classes. It maps the input to a probability distribution over the output classes, with each output value between 0 and 1 and the sum of all output values equal to 1.\n",
    "\n",
    "5. Leaky ReLU function: The leaky ReLU function is a variation of the ReLU function that addresses the \"dying ReLU\" problem, where some neurons become permanently inactive during training. The leaky ReLU function adds a small positive slope to the negative input region, allowing for non-zero outputs even for negative inputs.\n",
    "\n",
    "6. ELU function: The exponential linear unit (ELU) function is another variation of the ReLU function that addresses the \"dying ReLU\" problem. It has a smooth curve and allows negative input values, which helps to avoid dead neurons and improve performance. It is defined as: f(x) = x for x > 0 and f(x) = alpha * (e^x - 1) for x <= 0, where alpha is a small positive constant. \n",
    "\n",
    "These activation functions have different properties and are suitable for different types of neural network architectures and applications. The choice of activation function can significantly impact the performance and accuracy of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a3825",
   "metadata": {},
   "source": [
    "1.\tExplain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f8b96",
   "metadata": {},
   "source": [
    "\n",
    "Rosenblatt's perceptron model is one of the earliest and simplest models of an artificial neural network. It was developed in the late 1950s by Frank Rosenblatt and is a type of supervised learning algorithm for binary classification tasks.\n",
    "\n",
    "The perceptron model consists of a single layer of artificial neurons, also known as perceptrons. Each perceptron receives a set of input signals, multiplies them by a set of weights, and passes the weighted sum through an activation function to produce an output signal. The output signal is then compared to a threshold value, and the perceptron outputs a binary value of 1 or 0 depending on whether the threshold is exceeded or not.\n",
    "\n",
    "The perceptron learning algorithm involves iteratively adjusting the weights of the input signals to minimize the error between the predicted outputs and the actual outputs. The algorithm starts with random weights, and for each input data point, it computes the predicted output using the current weights. If the predicted output is correct, the weights are left unchanged. If the predicted output is incorrect, the weights are updated to bring the predicted output closer to the actual output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2dad0",
   "metadata": {},
   "source": [
    "2.\tUse a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808917e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To classify the given data points using a simple perceptron with weights w0=-1, w1=2, and w2=1, we need to compute the weighted sum of the inputs and pass it through the step activation function.\n",
    "\n",
    "For a given data point (x1, x2), the weighted sum is given by:\n",
    "\n",
    "weighted_sum = w0 + w1*x1 + w2*x2\n",
    "\n",
    "And the output of the perceptron is given by:\n",
    "\n",
    "output = 1 if weighted_sum > 0\n",
    "         0 if weighted_sum <= 0\n",
    "\n",
    "Using these equations, we can classify each data point as follows:\n",
    "\n",
    "For data point (3, 4):\n",
    "weighted_sum = -1 + 2*3 + 1*4 = 8\n",
    "output = 1 (since weighted_sum > 0)\n",
    "Therefore, (3, 4) is classified as positive.\n",
    "\n",
    "For data point (5, 2):\n",
    "weighted_sum = -1 + 2*5 + 1*2 = 11\n",
    "output = 1 (since weighted_sum > 0)\n",
    "Therefore, (5, 2) is classified as positive.\n",
    "\n",
    "For data point (1, -3):\n",
    "weighted_sum = -1 + 2*1 + 1*(-3) = -2\n",
    "output = 0 (since weighted_sum <= 0)\n",
    "Therefore, (1, -3) is classified as negative.\n",
    "\n",
    "For data point (-8, -3):\n",
    "weighted_sum = -1 + 2*(-8) + 1*(-3) = -20\n",
    "output = 0 (since weighted_sum <= 0)\n",
    "Therefore, (-8, -3) is classified as negative.\n",
    "\n",
    "For data point (-3, 0):\n",
    "weighted_sum = -1 + 2*(-3) + 1*0 = -7\n",
    "output = 0 (since weighted_sum <= 0)\n",
    "Therefore, (-3, 0) is classified as negative.\n",
    "\n",
    "Thus, the simple perceptron with weights w0=-1, w1=2, and w2=1 classifies the data points (3, 4) and (5, 2) as positive, and (1, -3), (-8, -3), and (-3, 0) as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbdcdb",
   "metadata": {},
   "source": [
    "\t3.Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c1810",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A multi-layer perceptron (MLP) is a type of neural network that consists of an input layer, one or more hidden layers, and an output layer. Each layer is composed of artificial neurons, and neurons in adjacent layers are fully connected, meaning that each neuron in one layer is connected to every neuron in the next layer. \n",
    "\n",
    "In an MLP, the output of each neuron in a layer is determined by a weighted sum of the outputs of the neurons in the previous layer, followed by the application of an activation function. The weights are adjusted during training to minimize the difference between the network's predicted outputs and the true outputs.\n",
    "\n",
    "The XOR problem is a classic example of a problem that cannot be solved by a single-layer perceptron. The XOR operation takes two binary inputs (0 or 1) and outputs 1 only if the inputs are different; otherwise, it outputs 0. The problem with a single-layer perceptron is that it can only separate input data that is linearly separable. However, the XOR problem requires a non-linear decision boundary to separate the input data.\n",
    "\n",
    "An MLP can solve the XOR problem by introducing a hidden layer between the input and output layers. The hidden layer provides a way for the network to learn non-linear decision boundaries. The weights of the network are adjusted during training to find the optimal decision boundary that separates the input data.\n",
    "\n",
    "For example, consider an MLP with one hidden layer consisting of two neurons, and an output layer consisting of one neuron. The input layer has two neurons that represent the two binary inputs to the XOR operation. The hidden layer applies an activation function (such as the sigmoid or ReLU function) to the weighted sum of the inputs, producing two outputs. These outputs are then fed to the output layer, which applies another activation function to produce the final output.\n",
    "\n",
    "During training, the weights of the network are adjusted using an algorithm such as backpropagation to minimize the error between the predicted outputs and the true outputs. With appropriate training, the MLP can learn to accurately classify inputs to the XOR operation, producing 0 or 1 as the output depending on whether the inputs are the same or different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb763a8",
   "metadata": {},
   "source": [
    "3. What is artificial neural network (ANN)? Explain some of the salient highlights in the\n",
    "different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d31e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "An artificial neural network (ANN) is a computational model that is inspired by the structure and function of biological neurons in the human brain. ANNs are used for various tasks, including pattern recognition, classification, prediction, and control.\n",
    "\n",
    "ANNs are composed of multiple layers of interconnected neurons that process and transmit information through the network. The neurons in each layer are connected to the neurons in the next layer, forming a feedforward network. The output of each neuron in a layer is determined by a weighted sum of the inputs, followed by the application of an activation function. The weights and biases of the neurons are adjusted during training to optimize the network's performance.\n",
    "\n",
    "Some salient highlights in the different architectural options for ANN are:\n",
    "\n",
    "1. Feedforward networks: In feedforward networks, the information flows only in one direction, from the input layer to the output layer, without any feedback loops. These networks are commonly used for tasks such as classification and regression.\n",
    "\n",
    "2. Recurrent networks: In recurrent networks, the information can flow in both directions, and the output of a neuron can feed back into the input of the same neuron or the neurons in previous layers. These networks are suitable for tasks that require sequential processing, such as speech recognition and language modeling.\n",
    "\n",
    "3. Convolutional networks: Convolutional networks are specialized feedforward networks that are designed to process data with a grid-like topology, such as images or time-series data. These networks use convolutional layers to extract features from the input data and pooling layers to reduce the dimensionality of the features.\n",
    "\n",
    "4. Autoencoders: Autoencoders are neural networks that are trained to learn a compressed representation of the input data, called the latent space. The network consists of an encoder that maps the input data to the latent space, and a decoder that maps the latent space back to the input data. Autoencoders are commonly used for tasks such as data compression and feature extraction.\n",
    "\n",
    "5. Generative networks: Generative networks are used to generate new data that is similar to the input data. These networks include generative adversarial networks (GANs) and variational autoencoders (VAEs), which are trained to generate realistic images, videos, or audio data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117bd11",
   "metadata": {},
   "source": [
    "4. Explain the learning process of an ANN. Explain, with example, the challenge in assigning\n",
    "synaptic weights for the interconnection between neurons? How can this challenge be\n",
    "addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd3fb3",
   "metadata": {},
   "source": [
    "The learning process of an Artificial Neural Network (ANN) involves adjusting the synaptic weights between neurons to improve the network's performance. The most common learning algorithms used in ANNs are supervised learning, unsupervised learning, and reinforcement learning.\n",
    "\n",
    "In supervised learning, the network is trained on a labeled dataset, where the input data is associated with the desired output. During training, the network adjusts the synaptic weights to minimize the difference between the predicted output and the desired output.\n",
    "\n",
    "In unsupervised learning, the network is trained on an unlabeled dataset, where the input data has no associated output. The network learns to identify patterns and structure in the input data by adjusting the synaptic weights.\n",
    "\n",
    "In reinforcement learning, the network learns to make decisions based on a reward signal that is received after each action. The network adjusts the synaptic weights to maximize the reward signal over time.\n",
    "\n",
    "The challenge in assigning synaptic weights for the interconnection between neurons is that there can be a large number of weights, and the optimal values for these weights are not known beforehand. The values of the weights can significantly impact the performance of the network. If the weights are set too high or too low, the network may not be able to learn the patterns in the data, leading to poor performance.\n",
    "\n",
    "One way to address this challenge is to use a random initialization of the weights. This method assigns random values to the synaptic weights, which allows the network to explore different weight configurations during training. Another approach is to use a pre-training step, where the network is first trained on a simpler task with a smaller dataset. This pre-training step can help the network learn useful representations of the input data, which can then be fine-tuned for the target task.\n",
    "\n",
    "For example, in a multi-layer perceptron trained to classify images, the synaptic weights between the input and hidden layers can be initialized randomly. During training, the weights are adjusted using a learning algorithm such as backpropagation. The challenge is to find the optimal values for the weights that minimize the error between the predicted and actual outputs. A common approach is to use a small learning rate and to update the weights iteratively over multiple epochs, until the network converges to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c564d",
   "metadata": {},
   "source": [
    "5. Explain, in details, the backpropagation algorithm. What are the limitations of this\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2a027",
   "metadata": {},
   "source": [
    "Backpropagation is a supervised learning algorithm used to train artificial neural networks. The goal of backpropagation is to adjust the synaptic weights between neurons in the network to minimize the error between the predicted output and the actual output.\n",
    "\n",
    "The backpropagation algorithm consists of two phases: the forward phase and the backward phase.\n",
    "\n",
    "In the forward phase, the input is propagated through the network layer by layer, and the output is calculated. The output of each neuron is computed as a weighted sum of its inputs, passed through an activation function.\n",
    "\n",
    "In the backward phase, the error between the predicted output and the actual output is propagated back through the network, and the weights are adjusted accordingly. The error is calculated as the difference between the predicted output and the actual output.\n",
    "\n",
    "The adjustment of weights is done by calculating the gradient of the error with respect to the weights using the chain rule of calculus. The gradient descent algorithm is used to update the weights in the direction of the negative gradient to minimize the error.\n",
    "\n",
    "The backpropagation algorithm is widely used in deep learning and has several advantages, such as its ability to handle complex problems and its scalability to large datasets. However, it also has some limitations:\n",
    "\n",
    "1. Backpropagation can get stuck in local minima: The optimization problem in backpropagation is non-convex, which means that there can be multiple local minima. Backpropagation may get stuck in a suboptimal local minimum, leading to poor performance.\n",
    "\n",
    "2. Backpropagation requires large amounts of data: The success of backpropagation depends on the availability of large amounts of labeled data. If the data is not representative or has too much noise, backpropagation may not be effective.\n",
    "\n",
    "3. Backpropagation is computationally expensive: The computation of gradients in backpropagation requires a lot of computational resources. Training large networks with many layers can take a long time and may require specialized hardware.\n",
    "\n",
    "4. Backpropagation is prone to overfitting: Backpropagation can fit the training data too closely, leading to overfitting. Regularization techniques such as dropout and weight decay can help address this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f65f4",
   "metadata": {},
   "source": [
    "6. Describe, in details, the process of adjusting the interconnection weights in a multi-layer\n",
    "neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b396366",
   "metadata": {},
   "source": [
    "The process of adjusting the interconnection weights in a multi-layer neural network involves two main steps: forward propagation and backpropagation. \n",
    "\n",
    "1. Forward Propagation:\n",
    "\n",
    "The forward propagation phase involves computing the output of each neuron in the network. The output of a neuron is computed by taking a weighted sum of its inputs and passing it through an activation function. The weights are initialized randomly, and the inputs to the network are propagated through the layers to compute the final output.\n",
    "\n",
    "2. Backpropagation:\n",
    "\n",
    "The backpropagation phase involves computing the error at the output layer and propagating it back through the network to adjust the weights. The error at the output layer is computed as the difference between the predicted output and the actual output.\n",
    "\n",
    "The error is then propagated back through the layers using the chain rule of calculus to compute the error at each neuron in the network. The error at each neuron is then used to update the weights of the incoming connections using the gradient descent algorithm.\n",
    "\n",
    "The gradient descent algorithm works by computing the gradient of the error with respect to the weights and updating the weights in the direction of the negative gradient. The learning rate determines the size of the weight update and is usually set to a small value to ensure convergence.\n",
    "\n",
    "The weight update rule is given by:\n",
    "\n",
    "new_weight = old_weight - learning_rate * gradient\n",
    "\n",
    "where the gradient is the partial derivative of the error with respect to the weight.\n",
    "\n",
    "This process is repeated for each input in the training dataset until the weights converge to a set of values that minimize the error between the predicted output and the actual output.\n",
    "\n",
    "It is important to note that the weights are updated after every forward pass and backpropagation step, and the process is repeated for multiple epochs until the error reaches a satisfactory level. During this process, the weights are adjusted to improve the network's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80abb8f",
   "metadata": {},
   "source": [
    "7. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is\n",
    "required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d641d1",
   "metadata": {},
   "source": [
    "The backpropagation algorithm is a popular optimization algorithm used for training neural networks. It works by computing the gradient of the loss function with respect to the weights of the neural network and adjusting the weights in the direction that minimizes the loss. The backpropagation algorithm can be broken down into the following steps:\n",
    "\n",
    "1. Forward Propagation: The inputs are propagated forward through the network, and the output is computed.\n",
    "\n",
    "2. Compute Error: The error is computed as the difference between the predicted output and the actual output.\n",
    "\n",
    "3. Backward Propagation: The error is propagated backward through the network to compute the gradient of the loss with respect to the weights.\n",
    "\n",
    "4. Update Weights: The weights are adjusted in the direction that minimizes the loss using the gradient descent algorithm.\n",
    "\n",
    "5. Repeat: The process is repeated for multiple epochs until the loss reaches a satisfactory level.\n",
    "\n",
    "A multi-layer neural network is required for the backpropagation algorithm because it enables the network to learn complex representations of the input data. A single-layer perceptron can only learn linearly separable patterns, while a multi-layer neural network with hidden layers can learn non-linear patterns. This is because the hidden layers introduce non-linearities into the model, allowing it to learn more complex representations of the input data.\n",
    "\n",
    "Furthermore, the backpropagation algorithm is well-suited for multi-layer neural networks because it can efficiently compute the gradient of the loss with respect to the weights using the chain rule of calculus. The algorithm propagates the error backward through the layers, computing the gradient at each layer and adjusting the weights accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3315be3",
   "metadata": {},
   "source": [
    "8. Write short notes on:\n",
    "\n",
    "    1.  Artificial neuron\n",
    "    2.  Multi-layer perceptron\n",
    "    3.  Deep learning\n",
    "    4.  Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723481a",
   "metadata": {},
   "source": [
    "1. Artificial Neuron: An artificial neuron, also called a perceptron, is a computational unit that receives input signals, processes them, and produces an output signal. It is modeled after the biological neuron in the brain and is the basic building block of artificial neural networks. An artificial neuron consists of multiple input connections, each with its weight, an activation function, and an output connection. The input signals are multiplied by their respective weights, summed up, and then passed through the activation function to produce an output signal.\n",
    "\n",
    "2. Multi-layer Perceptron: A multi-layer perceptron (MLP) is a type of artificial neural network that consists of multiple layers of interconnected artificial neurons. It is a feedforward network, which means that the data flows in one direction, from the input layer through the hidden layers to the output layer. MLPs can be used for a variety of tasks, including classification, regression, and pattern recognition. They are trained using the backpropagation algorithm, which adjusts the weights of the connections between the neurons to minimize the error between the predicted output and the actual output.\n",
    "\n",
    "3. Deep Learning: Deep learning is a subfield of machine learning that focuses on building artificial neural networks with multiple layers. Deep learning models can learn complex representations of the input data and have achieved state-of-the-art performance in a wide range of applications, including image recognition, natural language processing, and speech recognition. Deep learning models are trained using large datasets and require significant computational resources.\n",
    "\n",
    "4. Learning Rate: The learning rate is a hyperparameter that controls the size of the weight updates during the training of an artificial neural network. It determines how quickly the network adapts to the training data and can have a significant impact on the performance of the model. If the learning rate is too high, the model may overshoot the optimal weights and fail to converge, while if the learning rate is too low, the training process may be slow and the model may get stuck in a local minimum. The learning rate is usually set through trial and error and can be adjusted during the training process to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b07985",
   "metadata": {},
   "source": [
    "2. Write the difference between:-\n",
    "\n",
    "    1. Activation function vs threshold function\n",
    "    2. Step function vs sigmoid function\n",
    "    3. Single layer vs multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd77c60",
   "metadata": {},
   "source": [
    "1. Activation function vs Threshold function:\n",
    "An activation function is a mathematical function that is applied to the output of an artificial neuron to introduce nonlinearity into the model. It allows the artificial neuron to learn complex representations of the input data. Some popular activation functions used in deep learning are ReLU, sigmoid, and tanh. \n",
    "On the other hand, a threshold function is a binary function that returns 1 if the input is greater than a threshold value, and 0 otherwise. It is a simple linear model used for binary classification problems. The threshold function has no gradient and cannot be trained using backpropagation.\n",
    "\n",
    "2. Step function vs Sigmoid function:\n",
    "A step function is a binary function that returns 1 if the input is greater than or equal to a threshold value, and 0 otherwise. It is a simple linear model used for binary classification problems. However, it has some limitations as it is not continuous, differentiable, or smooth. \n",
    "A sigmoid function, on the other hand, is a smooth and continuous function that maps any input to a value between 0 and 1. It is commonly used as an activation function in artificial neural networks as it allows the model to learn non-linear decision boundaries. The sigmoid function has a smooth gradient, making it easy to compute the gradient of the error function during backpropagation.\n",
    "\n",
    "3. Single-layer vs Multi-layer Perceptron:\n",
    "A single-layer perceptron is a type of artificial neural network with only one layer of artificial neurons. It can only learn linearly separable patterns and is not suitable for complex tasks. It is a simple model used for linear regression or binary classification problems.\n",
    "A multi-layer perceptron, on the other hand, is a type of artificial neural network with multiple layers of artificial neurons. It can learn complex patterns and is suitable for a wide range of tasks, including classification, regression, and pattern recognition. It is a powerful model used in deep learning and can be trained using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb9452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
