{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a10acf8",
   "metadata": {},
   "source": [
    "1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a974f",
   "metadata": {},
   "source": [
    "CNNs (Convolutional Neural Networks) have several advantages over fully connected DNNs for image classification:\n",
    "\n",
    "1. **Parameter sharing:** In a fully connected DNN, each neuron is connected to every neuron in the previous layer, which results in an enormous number of parameters. In contrast, CNNs use a convolutional operation to share parameters across the input, which makes them more efficient and effective at learning local patterns and feature maps.\n",
    "\n",
    "2. **Translation invariance:** The convolutional operation in CNNs makes them invariant to translations in the input, which means they can recognize the same feature regardless of where it appears in the image. Fully connected DNNs, on the other hand, do not have this property.\n",
    "\n",
    "3. **Reduced dimensionality:** The convolutional operation reduces the dimensionality of the input, which makes it more manageable for subsequent layers. This also allows the model to generalize better to new images.\n",
    "\n",
    "4. **Hierarchical feature learning:** CNNs are designed to learn hierarchical representations of the input, with lower layers learning simple features like edges and corners, and higher layers learning more complex features like shapes and objects. This makes them well-suited for image classification tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad9673",
   "metadata": {},
   "source": [
    "2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5712d",
   "metadata": {},
   "source": [
    "To calculate the number of parameters in the CNN, we need to count the number of parameters in each layer and sum them up:\n",
    "\n",
    "- First convolutional layer: 3 × 3 kernel, 3 input channels, 100 output channels, plus 100 bias terms: (3 × 3 × 3 + 1) × 100 = 2,800 parameters\n",
    "- Second convolutional layer: 3 × 3 kernel, 100 input channels, 200 output channels, plus 200 bias terms: (3 × 3 × 100 + 1) × 200 = 180,200 parameters\n",
    "- Third convolutional layer: 3 × 3 kernel, 200 input channels, 400 output channels, plus 400 bias terms: (3 × 3 × 200 + 1) × 400 = 720,400 parameters\n",
    "\n",
    "The total number of parameters in the CNN is the sum of the parameters in each layer: 2,800 + 180,200 + 720,400 = 903,400 parameters.\n",
    "\n",
    "To calculate the RAM required for inference and training, we need to consider the size of each layer's output tensor and the size of the weight tensors, all in bytes. We assume that each float is 4 bytes.\n",
    "\n",
    "For inference on a single instance, the input tensor is 200 × 300 × 3 = 180,000 bytes. The output tensor of the first layer is 100 feature maps of size 100 × 150, which is 6,000,000 bytes. The output tensor of the second layer is 200 feature maps of size 50 × 75, which is 6,000,000 bytes. The output tensor of the third layer is 400 feature maps of size 25 × 38, which is 4,600,000 bytes. The total size of the weight tensors is 903,400 × 4 = 3,613,600 bytes. Therefore, the total RAM required for inference on a single instance is approximately 20,393,600 bytes or 20.39 MB.\n",
    "\n",
    "For training on a mini-batch of 50 images, we need to multiply the memory requirements by the batch size. Therefore, the total RAM required for training on a mini-batch of 50 images is approximately 1,019,680,000 bytes or 1.02 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3e165",
   "metadata": {},
   "source": [
    "3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc22862",
   "metadata": {},
   "source": [
    "If your GPU runs out of memory while training a CNN, here are five things you could try to solve the problem:\n",
    "\n",
    "1. Reduce the batch size: One common reason for running out of memory is that the batch size is too large. You can try reducing the batch size to fit the available memory. Note that smaller batch sizes may slow down training.\n",
    "\n",
    "2. Use a smaller model: If the model has too many parameters, it may not fit in memory. You can try using a smaller model with fewer layers or smaller layers to reduce the number of parameters.\n",
    "\n",
    "3. Use mixed precision training: Mixed precision training can reduce the memory usage by using lower precision (e.g., float16) for activations and gradients. This can increase the memory capacity and reduce the memory bandwidth requirements.\n",
    "\n",
    "4. Use gradient checkpointing: Gradient checkpointing is a technique that can reduce the memory usage by recomputing some intermediate activations during the backward pass. This can reduce the memory requirements at the cost of additional computation.\n",
    "\n",
    "5. Use data parallelism: Data parallelism is a technique that can distribute the computation across multiple GPUs. Each GPU processes a subset of the data, and the gradients are aggregated across all GPUs. This can increase the memory capacity and reduce the memory bandwidth requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acdb4b",
   "metadata": {},
   "source": [
    "4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604e0ff",
   "metadata": {},
   "source": [
    "Adding a max pooling layer rather than a convolutional layer with the same stride has several advantages:\n",
    "\n",
    "1. Reduction of spatial dimensions: Max pooling reduces the spatial dimensions of the feature maps, while retaining the most important features. This leads to a smaller number of parameters in subsequent layers, and a reduced risk of overfitting.\n",
    "\n",
    "2. Invariance to small translations: Max pooling provides a degree of invariance to small translations in the input, which can be useful for tasks such as object recognition.\n",
    "\n",
    "3. Non-linearity: Max pooling introduces a non-linearity into the network, which can improve its ability to model complex functions.\n",
    "\n",
    "4. Computational efficiency: Max pooling is computationally more efficient than convolution with the same stride, since it does not require any multiplication operations.\n",
    "\n",
    "Overall, adding a max pooling layer can help improve the efficiency and effectiveness of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b1285",
   "metadata": {},
   "source": [
    "5.\tWhen would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216fb17",
   "metadata": {},
   "source": [
    "Local response normalization (LRN) is a technique that can be used to improve the generalization of a CNN model. It is typically added after a convolutional layer and before a pooling layer. The purpose of LRN is to introduce competition between feature maps and prevent overfitting. Specifically, it works by normalizing the responses of a group of neurons across nearby channels (feature maps) at each spatial location. \n",
    "\n",
    "The main benefit of LRN is to improve generalization, particularly in cases where the model may be overfitting to the training set. However, it is worth noting that LRN has largely fallen out of favor in recent years, as other techniques such as batch normalization have been shown to be more effective and efficient. Therefore, in practice, LRN is not commonly used anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bd46c",
   "metadata": {},
   "source": [
    "6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bcf83",
   "metadata": {},
   "source": [
    "\n",
    "- **AlexNet:** This model was introduced in 2012 and achieved state-of-the-art performance on the ImageNet dataset. It was notable for its use of ReLU activation functions, which helped to address the vanishing gradient problem, and for its use of data augmentation techniques to increase the effective size of the training set. It also introduced the concept of using GPUs to accelerate deep learning training, which is now standard practice.\n",
    "\n",
    "- **GoogLeNet:** This model was introduced in 2014 and won the ImageNet challenge that year. It is notable for its use of \"inception modules,\" which are blocks of layers that process the input in parallel at multiple scales and then concatenate the output. This allowed for a significant increase in model depth and accuracy without adding too many parameters. The GoogLeNet architecture also introduced the use of global average pooling instead of fully connected layers, which helped to reduce overfitting.\n",
    "\n",
    "- **ResNet:** This model was introduced in 2015 and is notable for its use of \"residual connections,\" which allow for deeper networks to be trained without suffering from the vanishing gradient problem. The residual connections essentially bypass some layers in the network and feed the input directly to later layers, which allows gradients to flow more easily through the network.\n",
    "\n",
    "- **SENet:** This model was introduced in 2017 and is notable for its use of \"squeeze and excitation\" blocks, which are used to recalibrate channel-wise feature responses based on their relevance to the task at hand. This helps to improve model accuracy while reducing the number of parameters.\n",
    "\n",
    "- **Xception:** This model was introduced in 2017 and is notable for its use of \"depthwise separable convolutions,\" which split the standard convolution operation into two separate operations: a depthwise convolution that applies a single filter to each input channel separately, followed by a pointwise convolution that combines the output of the depthwise convolution using 1x1 convolutions. This allows for a significant reduction in the number of parameters without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815ec81",
   "metadata": {},
   "source": [
    "7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a443d9",
   "metadata": {},
   "source": [
    "A fully convolutional network (FCN) is a neural network that consists entirely of convolutional layers, with no dense layers at the end. The main advantage of FCNs over regular CNNs is that they can take input images of any size, since the output of a convolutional layer only depends on the size of its input and the size of its filter. This makes FCNs well-suited for tasks such as semantic segmentation, where the goal is to predict a class label for each pixel in an input image.\n",
    "\n",
    "To convert a dense layer into a convolutional layer, we need to reshape the weights of the dense layer into the shape of a convolutional filter. For example, suppose we have a dense layer with 100 neurons, and we want to convert it into a convolutional layer with a filter size of 5x5 and 20 output channels. We could reshape the weight matrix into a 5x5x100 tensor, and then use that as the convolutional filter with 20 output channels. The resulting convolutional layer would take a 4D tensor as input (batch size, height, width, input channels) and output a tensor with shape (batch size, height - 4, width - 4, 20). Note that we subtract 4 from the height and width because the convolutional filter has a size of 5x5, which reduces the size of the output by 4 in each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f619d8",
   "metadata": {},
   "source": [
    "8.\tWhat is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1eae8",
   "metadata": {},
   "source": [
    "The main technical difficulty of semantic segmentation is to accurately classify each pixel in an image while preserving the spatial information. This is a more challenging task than image classification, where the goal is to classify an entire image into one of several categories. In semantic segmentation, the output must be a dense classification map, with each pixel in the input image mapped to a corresponding pixel in the output map. This requires capturing both local and global context information, handling varying object scales and orientations, dealing with occlusions and overlapping objects, and ensuring smoothness of the output map. Additionally, large amounts of labeled data are typically required to train accurate segmentation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c69c67",
   "metadata": {},
   "source": [
    "9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eebc78",
   "metadata": {},
   "source": [
    " here's an example of building a CNN from scratch using Keras to achieve high accuracy on the MNIST dataset:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "```\n",
    "\n",
    "This model consists of two convolutional layers with 32 and 64 filters respectively, followed by max pooling layers to reduce the spatial dimensions of the feature maps. The output of the second max pooling layer is then flattened and passed through two dense layers with 128 and 10 units respectively, with the final layer using the softmax activation function to produce class probabilities.\n",
    "\n",
    "When trained on the MNIST dataset, this model achieves an accuracy of over 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba5cf1",
   "metadata": {},
   "source": [
    "10.\tUse transfer learning for large image classification, going through these steps:\n",
    " a.\tCreate a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    " \n",
    " b.\tSplit it into a training set, a validation set, and a test set.\n",
    " \n",
    " c.\tBuild the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.\n",
    " \n",
    " d.\tFine-tune a pretrained model on this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426057f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
